{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "import face_recognition_models\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norwegian-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\n",
    "pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n",
    "\n",
    "face_recognition_model = face_recognition_models.face_recognition_model_location()\n",
    "face_encoder = dlib.face_recognition_model_v1(face_recognition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neutral-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_file(file):\n",
    "    im = PIL.Image.open(file)\n",
    "    im = im.convert('RGB')\n",
    "    return np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "through-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_to_rect(css):\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])\n",
    "def raw_face_location(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "    \n",
    "    if model == \"cnn\":\n",
    "        return cnn_face_detector(img, number_of_times_to_upsample)\n",
    "    else:\n",
    "        face_detector = dlib.get_frontal_face_detector()\n",
    "        return face_detector(img, number_of_times_to_upsample)\n",
    "def raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
    "    if face_locations is None:\n",
    "        face_locations = raw_face_location(face_image)\n",
    "    else:\n",
    "        face_locations = [css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "    pose_predictor = pose_predictor_5_point\n",
    "\n",
    "\n",
    "    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n",
    "def encode_face(face_image, known_face_locations=None, num_jitters=1, model=\"small\"):\n",
    "    \n",
    "    raw_landmarks = raw_face_landmarks(face_image, known_face_locations, model)\n",
    "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extreme-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_image = load_image_file(\"obama.jpg\")\n",
    "obama_face_encoding = encode_face(obama_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lucky-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = [\n",
    "    obama_face_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Barack Obama\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-timeline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "injured-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_css(rect):\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()\n",
    "\n",
    "def trim_css_to_bounds(css, image_shape):\n",
    "\n",
    "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
    "\n",
    "def get_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "\n",
    "    if model == \"cnn\":\n",
    "        return [trim_css_to_bounds(rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n",
    "    else:\n",
    "        return [trim_css_to_bounds(rect_to_css(face), img.shape) for face in raw_face_location(img, number_of_times_to_upsample, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_distance(face_encodings, face_to_compare):\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
    "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n",
    "    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "billion-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while(True):\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Our operations on the frame come here\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('frame',gray)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When everything done, release the capture\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    frameorg= frame\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = get_face_locations(rgb_frame)\n",
    "    face_encodings = encode_face(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through each face in this frame of video\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        # if True in matches:\n",
    "        #     first_match_index = matches.index(True)\n",
    "        #     name = known_face_names[first_match_index]\n",
    "\n",
    "        # Or instead, use the known face with the smallest distance to the new face\n",
    "        face_distances = face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frameorg)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-power",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-judge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
