{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "import face_recognition_models\n",
    "import cv2\n",
    "import numpy\n",
    "import threading\n",
    "import platform\n",
    "from multiprocessing import Process, Manager, cpu_count, set_start_method\n",
    "import time\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divided-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next worker's id\n",
    "def next_id(current_id, worker_num):\n",
    "    if current_id == worker_num:\n",
    "        return 1\n",
    "    else:\n",
    "        return current_id + 1\n",
    "\n",
    "\n",
    "# Get previous worker's id\n",
    "def prev_id(current_id, worker_num):\n",
    "    if current_id == 1:\n",
    "        return worker_num\n",
    "    else:\n",
    "        return current_id - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solved-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A subprocess use to capture frames.\n",
    "def capture(read_frame_list, Global, worker_num):\n",
    "    # Get a reference to webcam #0 (the default one)\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    # video_capture.set(3, 640)  # Width of the frames in the video stream.\n",
    "    # video_capture.set(4, 480)  # Height of the frames in the video stream.\n",
    "    # video_capture.set(5, 30) # Frame rate.\n",
    "    print(\"Width: %d, Height: %d, FPS: %d\" % (video_capture.get(3), video_capture.get(4), video_capture.get(5)))\n",
    "\n",
    "    while not Global.is_exit:\n",
    "        # If it's time to read a frame\n",
    "        if Global.buff_num != next_id(Global.read_num, worker_num):\n",
    "            # Grab a single frame of video\n",
    "            ret, frame = video_capture.read()\n",
    "            read_frame_list[Global.buff_num] = frame\n",
    "            Global.buff_num = next_id(Global.buff_num, worker_num)\n",
    "        else:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    # Release webcam\n",
    "    video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eligible-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norwegian-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\n",
    "pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n",
    "\n",
    "face_recognition_model = face_recognition_models.face_recognition_model_location()\n",
    "face_encoder = dlib.face_recognition_model_v1(face_recognition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neutral-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_file(file):\n",
    "    im = PIL.Image.open(file)\n",
    "    im = im.convert('RGB')\n",
    "    return np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "through-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def css_to_rect(css):\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])\n",
    "def raw_face_location(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "    \n",
    "    if model == \"cnn\":\n",
    "        return cnn_face_detector(img, number_of_times_to_upsample)\n",
    "    else:\n",
    "        face_detector = dlib.get_frontal_face_detector()\n",
    "        return face_detector(img, number_of_times_to_upsample)\n",
    "def raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
    "    if face_locations is None:\n",
    "        face_locations = raw_face_location(face_image)\n",
    "    else:\n",
    "        face_locations = [css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "    pose_predictor = pose_predictor_5_point\n",
    "\n",
    "\n",
    "    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n",
    "def encode_face(face_image, known_face_locations=None, num_jitters=1, model=\"small\"):\n",
    "    \n",
    "    raw_landmarks = raw_face_landmarks(face_image, known_face_locations, model)\n",
    "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extreme-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes each image from the folder and encodes each of them as known face encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "for image in glob.glob('./images/*'):\n",
    "    img = load_image_file(image)\n",
    "    enc = encode_face(img)[0]\n",
    "    known_face_encodings.append(enc)\n",
    "    known_face_names.append(os.path.splitext(os.path.basename(image))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "other-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_css(rect):\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()\n",
    "\n",
    "def trim_css_to_bounds(css, image_shape):\n",
    "\n",
    "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
    "\n",
    "def get_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "\n",
    "    if model == \"cnn\":\n",
    "        return [trim_css_to_bounds(rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n",
    "    else:\n",
    "        return [trim_css_to_bounds(rect_to_css(face), img.shape) for face in raw_face_location(img, number_of_times_to_upsample, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offensive-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_distance(face_encodings, face_to_compare):\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
    "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n",
    "    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adopted-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many subprocess use to process frames.\n",
    "def process(worker_id, read_frame_list, write_frame_list, Global, worker_num):\n",
    "    known_face_encodings = Global.known_face_encodings\n",
    "    known_face_names = Global.known_face_names\n",
    "    while not Global.is_exit:\n",
    "\n",
    "        # Wait to read\n",
    "        while Global.read_num != worker_id or Global.read_num != prev_id(Global.buff_num, worker_num):\n",
    "            # If the user has requested to end the app, then stop waiting for webcam frames\n",
    "            if Global.is_exit:\n",
    "                break\n",
    "\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        # Delay to make the video look smoother\n",
    "        time.sleep(Global.frame_delay)\n",
    "\n",
    "        # Read a single frame from frame list\n",
    "        frame_process = read_frame_list[worker_id]\n",
    "\n",
    "        # Expect next worker to read frame\n",
    "        Global.read_num = next_id(Global.read_num, worker_num)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_frame = frame_process[:, :, ::-1]\n",
    "\n",
    "        # Find all the faces and face encodings in the frame of video, cost most time\n",
    "        face_locations = get_face_locations(rgb_frame)\n",
    "        face_encodings = face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        # Loop through each face in this frame of video\n",
    "        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # If a match was found in known_face_encodings, just use the first one.\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame_process, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame_process, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame_process, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        # Wait to write\n",
    "        while Global.write_num != worker_id:\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        # Send frame to global\n",
    "        write_frame_list[worker_id] = frame_process\n",
    "\n",
    "        # Expect next worker to write frame\n",
    "        Global.write_num = next_id(Global.write_num, worker_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-actor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "blessed-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker number 15\n",
      "[<Thread(Thread-6, started 20068)>, <Process(Process-10, started)>, <Process(Process-11, started)>, <Process(Process-12, started)>, <Process(Process-13, started)>, <Process(Process-14, started)>, <Process(Process-15, started)>, <Process(Process-16, started)>, <Process(Process-17, started)>, <Process(Process-18, started)>, <Process(Process-19, started)>, <Process(Process-20, started)>, <Process(Process-21, started)>, <Process(Process-22, started)>, <Process(Process-23, started)>, <Process(Process-24, started)>]\n",
      "Width: 640, Height: 480, FPS: 30\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # Global variables\n",
    "    Global = Manager().Namespace()\n",
    "    Global.buff_num = 1\n",
    "    Global.read_num = 1\n",
    "    Global.write_num = 1\n",
    "    Global.frame_delay = 0\n",
    "    Global.is_exit = False\n",
    "    read_frame_list = Manager().dict()\n",
    "    write_frame_list = Manager().dict()\n",
    "\n",
    "    # Number of workers (subprocess use to process frames)\n",
    "    if cpu_count() > 2:\n",
    "        worker_num = cpu_count() - 1  # 1 for capturing frames\n",
    "    else:\n",
    "        worker_num = 2\n",
    "    print(f'worker number {worker_num}')\n",
    "    # Subprocess list\n",
    "    p = []\n",
    "\n",
    "    # Create a thread to capture frames (if uses subprocess, it will crash on Mac)\n",
    "    p.append(threading.Thread(target=capture, args=(read_frame_list, Global, worker_num,)))\n",
    "    p[0].start()\n",
    "\n",
    "    # Create workers\n",
    "    for worker_id in range(1, worker_num + 1):\n",
    "        p.append(Process(target=process, args=(worker_id, read_frame_list, write_frame_list, Global, worker_num,)))\n",
    "        p[worker_id].start()\n",
    "    print(p)\n",
    "\n",
    "#     # Start to show video\n",
    "#     last_num = 1\n",
    "#     fps_list = []\n",
    "#     tmp_time = time.time()\n",
    "#     while not Global.is_exit:\n",
    "#         while Global.write_num != last_num:\n",
    "#             last_num = int(Global.write_num)\n",
    "\n",
    "#             # Calculate fps\n",
    "#             delay = time.time() - tmp_time\n",
    "#             tmp_time = time.time()\n",
    "#             fps_list.append(delay)\n",
    "#             if len(fps_list) > 5 * worker_num:\n",
    "#                 fps_list.pop(0)\n",
    "#             fps = len(fps_list) / numpy.sum(fps_list)\n",
    "#             print(\"fps: %.2f\" % fps)\n",
    "\n",
    "#             # Calculate frame delay, in order to make the video look smoother.\n",
    "#             # When fps is higher, should use a smaller ratio, or fps will be limited in a lower value.\n",
    "#             # Larger ratio can make the video look smoother, but fps will hard to become higher.\n",
    "#             # Smaller ratio can make fps higher, but the video looks not too smoother.\n",
    "#             # The ratios below are tested many times.\n",
    "#             if fps < 6:\n",
    "#                 Global.frame_delay = (1 / fps) * 0.75\n",
    "#             elif fps < 20:\n",
    "#                 Global.frame_delay = (1 / fps) * 0.5\n",
    "#             elif fps < 30:\n",
    "#                 Global.frame_delay = (1 / fps) * 0.25\n",
    "#             else:\n",
    "#                 Global.frame_delay = 0\n",
    "\n",
    "#             # Display the resulting image\n",
    "#             cv2.imshow('Video', write_frame_list[prev_id(Global.write_num, worker_num)])\n",
    "\n",
    "#         # Hit 'q' on the keyboard to quit!\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             Global.is_exit = True\n",
    "#             break\n",
    "\n",
    "#         time.sleep(0.01)\n",
    "\n",
    "#     # Quit\n",
    "#     video_capture.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_capture = cv2.VideoCapture(0)\n",
    "# while(True):\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = video_capture.read()\n",
    "#     frameorg= frame\n",
    "\n",
    "#     # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "#     rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "#     # Find all the faces and face enqcodings in the frame of video\n",
    "#     face_locations = get_face_locations(rgb_frame)\n",
    "#     face_encodings = encode_face(rgb_frame, face_locations)\n",
    "\n",
    "#     # Loop through each face in this frame of video\n",
    "#     for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "#         # See if the face is a match for the known face(s)\n",
    "#         matches = compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "#         name = \"Unknown\"\n",
    "\n",
    "#         # If a match was found in known_face_encodings, just use the first one.\n",
    "#         # if True in matches:\n",
    "#         #     first_match_index = matches.index(True)\n",
    "#         #     name = known_face_names[first_match_index]\n",
    "\n",
    "#         # Or instead, use the known face with the smallest distance to the new face\n",
    "#         face_distances = face_distance(known_face_encodings, face_encoding)\n",
    "#         best_match_index = np.argmin(face_distances)\n",
    "#         if matches[best_match_index]:\n",
    "#             name = known_face_names[best_match_index]\n",
    "\n",
    "#         # Draw a box around the face\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "#         # Draw a label with a name below the face\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "#     # Display the resulting image\n",
    "#     cv2.imshow('Video', frameorg)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-power",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-appointment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-aviation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
