{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "import face_recognition_models\n",
    "import cv2\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eligible-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "norwegian-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis object maps human faces into 128D vectors where pictures of the same person are mapped near to \\neach other and pictures of different people are mapped far apart. The constructor loads the face \\nrecognition model from a file\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrained weights of models used in dlib\n",
    "\n",
    "predictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\n",
    "pose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n",
    "'''\n",
    "This object is a tool that takes in an image region containing some object and outputs a set of point \n",
    "locations that define the pose of the object. The classic example of this is human face pose prediction,\n",
    "where you take an image of a human face as input and are expected to identify the locations of important \n",
    "facial landmarks such as the corners of the mouth and eyes, tip of the nose, and so forth.\n",
    "'''\n",
    "\n",
    "face_recognition_model = face_recognition_models.face_recognition_model_location()\n",
    "face_encoder = dlib.face_recognition_model_v1(face_recognition_model)\n",
    "\n",
    "'''\n",
    "This object maps human faces into 128D vectors where pictures of the same person are mapped near to \n",
    "each other and pictures of different people are mapped far apart. The constructor loads the face \n",
    "recognition model from a file\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neutral-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_file(file):\n",
    "    im = PIL.Image.open(file)\n",
    "    im = im.convert('RGB')\n",
    "    return np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfactory-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object represents a rectangular area of an image.\n",
    "\n",
    "def css_to_rect(css):\n",
    "    return dlib.rectangle(css[3], css[0], css[1], css[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powered-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default face detector object to locate the face locations\n",
    "\n",
    "def raw_face_location(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    return face_detector(img, number_of_times_to_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "popular-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the 5 point face location estimate using dlib libraries\n",
    "\n",
    "def raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n",
    "    if face_locations is None:\n",
    "        face_locations = raw_face_location(face_image)\n",
    "    else:\n",
    "        face_locations = [css_to_rect(face_location) for face_location in face_locations]\n",
    "\n",
    "    pose_predictor = pose_predictor_5_point\n",
    "\n",
    "\n",
    "    return [pose_predictor(face_image, face_location) for face_location in face_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "through-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes face into 128D vector\n",
    "\n",
    "def encode_face(face_image, known_face_locations=None, num_jitters=1, model=\"small\"):\n",
    "    \n",
    "    raw_landmarks = raw_face_landmarks(face_image, known_face_locations, model)\n",
    "    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lucky-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes each image from the folder and encodes each of them as known face encodings\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "for image in glob.glob('./images/*'):\n",
    "    img = load_image_file(image)\n",
    "    enc = encode_face(img)[0]\n",
    "    known_face_encodings.append(enc)\n",
    "    known_face_names.append(os.path.splitext(os.path.basename(image))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "overhead-cocktail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elon', 'Obama', 'Sanjay', 'TIm']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "injured-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dlib rect format to css\n",
    "def rect_to_css(rect):\n",
    "    return rect.top(), rect.right(), rect.bottom(), rect.left()\n",
    "\n",
    "#Trim excess bounds to get the face location\n",
    "def trim_css_to_bounds(css, image_shape):\n",
    "\n",
    "    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n",
    "\n",
    "#inputs and data and returns the face locations by calling raw face location function\n",
    "def get_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n",
    "\n",
    "    return [trim_css_to_bounds(rect_to_css(face), img.shape) for face in raw_face_location(img, number_of_times_to_upsample, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "floral-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the vector difference between the known 128D vector and the unknown vector\n",
    "\n",
    "def face_distance(face_encodings, face_to_compare):\n",
    "    if len(face_encodings) == 0:\n",
    "        return np.empty((0))\n",
    "\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n",
    "\n",
    "# list out all the face encoding for a certain threshold\n",
    "\n",
    "def compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n",
    "    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "billion-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while(True):\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # Our operations on the frame come here\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('frame',gray)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # When everything done, release the capture\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "written-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while(True):\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    frameorg= frame\n",
    "    \n",
    "    \n",
    "    # Converting from BGR to RGB\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = get_face_locations(rgb_frame)\n",
    "    face_encodings = encode_face(rgb_frame, face_locations)\n",
    "    \n",
    "    #draw face locations\n",
    "    for loc in face_locations:\n",
    "        cv2.rectangle(frameorg, (loc[1], loc[0]), (loc[3], loc[2]), (0, 0, 255), 2)\n",
    "    cv2.imshow('original fram',frameorg)\n",
    "    \n",
    "    # Loop through each face in this frame of video\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # human face with the smallest distance but which passes a default threshold is selected\n",
    "        face_distances = face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frameorg)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unlimited-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\sanju\\anaconda3\\envs\\face_rec\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consistent-power",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-faabb92d6449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(8, 8))\n",
    "rows = min(4,len(face_encodings))\n",
    "columns = len(face_encodings)//4 +1\n",
    "for i in range(columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(face_encodings[i].reshape(16,8), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-collection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
